{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis\n",
    "    The \"Churn_Modelling.csv\" dataset contains information about customers of a bank. \n",
    "    Problem Statement: The bank has been seeing unusual churn rate and they want to assess and address the problem.\n",
    "    \n",
    "    The bank has released information related to 10,000 customers from European region.\n",
    "    GOAL: our job is to create a GeoDemographic Segmentation Model to help the bank identify which customers are at the highest risk of leaving. \n",
    "    \n",
    "    *Geo Demographic Segmentation have lots of application in determining decision making based on Geo Demographic Scenarios.\n",
    "    \n",
    "    Observation: The outcome is binary in nature.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at data. Understanding the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RowNumber    CustomerId   CreditScore           Age        Tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  10000.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800      5.012800   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806      2.892174   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000      0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000      3.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000      5.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000      7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000     10.000000   \n",
       "\n",
       "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
       "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
       "mean    76485.889288       1.530200      0.70550        0.515100   \n",
       "std     62397.405202       0.581654      0.45584        0.499797   \n",
       "min         0.000000       1.000000      0.00000        0.000000   \n",
       "25%         0.000000       1.000000      0.00000        0.000000   \n",
       "50%     97198.540000       1.000000      1.00000        1.000000   \n",
       "75%    127644.240000       2.000000      1.00000        1.000000   \n",
       "max    250898.090000       4.000000      1.00000        1.000000   \n",
       "\n",
       "       EstimatedSalary        Exited  \n",
       "count     10000.000000  10000.000000  \n",
       "mean     100090.239881      0.203700  \n",
       "std       57510.492818      0.402769  \n",
       "min          11.580000      0.000000  \n",
       "25%       51002.110000      0.000000  \n",
       "50%      100193.915000      0.000000  \n",
       "75%      149388.247500      0.000000  \n",
       "max      199992.480000      1.000000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,3:13].values\n",
    "Y = data.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'numpy.ndarray'>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder_X_1 = LabelEncoder()\n",
    "X[:,1] = labelEncoder_X_1.fit_transform(X[:,1])\n",
    "labelEncoder_X_2 = LabelEncoder()\n",
    "X[:,2] = labelEncoder_X_2.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([619, 0, 0, 42, 2, 0.0, 1, 1, 1, 101348.88], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(categorical_features=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = oneHotEncoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[:,1:]\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8000)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test),len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling \n",
    "    Feature Scaling of X_train and X_test(REQUIRED to ease on the large amount of computations)\n",
    "    Artificial Neural Networks are very compute intensive. Feature scaling makes it easy doing the computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Making the ANN using the data\n",
    "    Will require Keras Library and some modules within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need 2 modules within Keras\n",
    "    1> Sequential Module - To Initialize the ANN\n",
    "    2> Dense Module - To build layer of ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We initialize the Deep learning model by defining it as a sequence of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rahul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "/Users/Rahul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "/Users/Rahul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n"
     ]
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the Input Layers and the first hidden Layer\n",
    "\"\"\"\n",
    "Adding number of layers is said to be an art. It requires experimentation\n",
    "\n",
    "But a goot TIP is to:\n",
    "No. of Nodes in the Hidden layer = Average(No. of Nodes in InputLayer,No. of Nodes in OutputLayer) \n",
    "x  = (11 + 1)/2 = 6 Nodes\n",
    "\n",
    "Input Dimensions = 11\n",
    "We need to specify it in the Dense() for the First Hidden Layer\n",
    "\n",
    "\n",
    "Otherwise,\n",
    "We can use K-fold CrossValidation to determine it\n",
    "\"\"\"\n",
    "\n",
    "# Adding the input layer and the first hidden layer (Input Dim required for First Hidden Layer)\n",
    "# InputLayer + Hidden Layer 1\n",
    "classifier.add(Dense(output_dim = 6,init = \"uniform\",activation=\"relu\",input_dim = 11))\n",
    "\n",
    "# Hidden Layer 2 \n",
    "classifier.add(Dense(output_dim = 6,init = \"uniform\",activation = \"relu\"))\n",
    "\n",
    "# Output Layer (Activation=\"sigmoid\" and output_dim = 1,  because O/p is Binary) \n",
    "\n",
    "# If dependent variable has more than 2 catergories, Use SOFTMAX \n",
    "classifier.add(Dense(output_dim = 1,init = \"uniform\",activation = \"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the ANN\n",
    "    Now that we have added all the layers, we move towards compiling\n",
    "    Applying Stochastic Gradient Descent for the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module keras.engine.training:\n",
      "\n",
      "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    # Arguments\n",
      "        optimizer: String (name of optimizer) or optimizer instance.\n",
      "            See [optimizers](/optimizers).\n",
      "        loss: String (name of objective function) or objective function.\n",
      "            See [losses](/losses).\n",
      "            If the model has multiple outputs, you can use a different loss\n",
      "            on each output by passing a dictionary or a list of losses.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the sum of all individual losses.\n",
      "        metrics: List of metrics to be evaluated by the model\n",
      "            during training and testing.\n",
      "            Typically you will use `metrics=['accuracy']`.\n",
      "            To specify different metrics for different outputs of a\n",
      "            multi-output model, you could also pass a dictionary,\n",
      "            such as `metrics={'output_a': 'accuracy'}`.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "            coefficients (Python floats) to weight the loss contributions\n",
      "            of different model outputs.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the *weighted sum* of all individual losses,\n",
      "            weighted by the `loss_weights` coefficients.\n",
      "            If a list, it is expected to have a 1:1 mapping\n",
      "            to the model's outputs. If a tensor, it is expected to map\n",
      "            output names (strings) to scalar coefficients.\n",
      "        sample_weight_mode: If you need to do timestep-wise\n",
      "            sample weighting (2D weights), set this to `\"temporal\"`.\n",
      "            `None` defaults to sample-wise weights (1D).\n",
      "            If the model has multiple outputs, you can use a different\n",
      "            `sample_weight_mode` on each output by passing a\n",
      "            dictionary or a list of modes.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted\n",
      "            by sample_weight or class_weight during training and testing.\n",
      "        target_tensors: By default, Keras will create placeholders for the\n",
      "            model's target, which will be fed with the target data during\n",
      "            training. If instead you would like to use your own\n",
      "            target tensors (in turn, Keras will not expect external\n",
      "            Numpy data for these targets at training time), you\n",
      "            can specify them via the `target_tensors` argument. It can be\n",
      "            a single tensor (for a single-output model), a list of tensors,\n",
      "            or a dict mapping output names to target tensors.\n",
      "        **kwargs: When using the Theano/CNTK backends, these arguments\n",
      "            are passed into `K.function`.\n",
      "            When using the TensorFlow backend,\n",
      "            these arguments are passed into `tf.Session.run`.\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: In case of invalid arguments for\n",
      "            `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very efficient type of the Stochastic Gradient Algo is called \"adam\"\n",
    "# Loss function for Output Sigmoid \n",
    "#       Loss for Binary Output = binary_crossentropy\n",
    "#       Loss for Categorical Output = Categorical_CrossEntropy\n",
    "classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the ANN to the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.4427 - acc: 0.7959\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 7s 899us/step - loss: 0.4210 - acc: 0.8188\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 7s 895us/step - loss: 0.4155 - acc: 0.8294\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 7s 904us/step - loss: 0.4125 - acc: 0.8324\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 7s 895us/step - loss: 0.4109 - acc: 0.8325\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 7s 915us/step - loss: 0.4089 - acc: 0.8337\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 8s 966us/step - loss: 0.4085 - acc: 0.8307\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.4081 - acc: 0.8347A: 0s - loss: 0.4087 - acc:\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 7s 911us/step - loss: 0.4068 - acc: 0.8353\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 7s 928us/step - loss: 0.4063 - acc: 0.8311\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 8s 951us/step - loss: 0.4067 - acc: 0.8356 1s - loss: 0.4042  - ETA: 1s - loss: 0.4085 - acc:  - ETA: 0s - loss: 0\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4054 - acc: 0.8326\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4046 - acc: 0.8360\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4053 - acc: 0.8332\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4053 - acc: 0.8336\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4052 - acc: 0.8337\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4042 - acc: 0.8340\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.4046 - acc: 0.8336\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.4039 - acc: 0.8359\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 7s 918us/step - loss: 0.4045 - acc: 0.8345 0s - loss: 0.4\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 8s 938us/step - loss: 0.4041 - acc: 0.8315 3s - loss: 0.4052 - acc - E - ETA: 1s - l\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 7s 909us/step - loss: 0.4044 - acc: 0.8336\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 7s 935us/step - loss: 0.4045 - acc: 0.8351\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 7s 921us/step - loss: 0.4035 - acc: 0.8330\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 7s 927us/step - loss: 0.4034 - acc: 0.8341 4s - loss: 0.4083 - acc: 0.83 - ETA: 4s - loss: 0.4106 - acc: 0.8 - ETA: 1s - loss: 0.4110 - acc - ETA: 1\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 7s 913us/step - loss: 0.4029 - acc: 0.8353\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 8s 941us/step - loss: 0.4035 - acc: 0.8344\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 8s 958us/step - loss: 0.4033 - acc: 0.8349 0s - loss: 0.3999 -\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 8s 952us/step - loss: 0.4030 - acc: 0.8347\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 8s 984us/step - loss: 0.4030 - acc: 0.8332\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 7s 934us/step - loss: 0.4028 - acc: 0.8340\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 8s 975us/step - loss: 0.4034 - acc: 0.8329 1s - l\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 7s 929us/step - loss: 0.4035 - acc: 0.8356 0s - loss: 0.4070\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 7s 934us/step - loss: 0.4037 - acc: 0.8331\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 7s 927us/step - loss: 0.4037 - acc: 0.8335\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 7s 932us/step - loss: 0.4029 - acc: 0.8334\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 8s 958us/step - loss: 0.4033 - acc: 0.8321 2s - loss: 0.3962 - acc: 0.8 - \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 8s 948us/step - loss: 0.4032 - acc: 0.8326\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 8s 957us/step - loss: 0.4028 - acc: 0.8345\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 8s 958us/step - loss: 0.4038 - acc: 0.8331\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 8s 972us/step - loss: 0.4030 - acc: 0.8361\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 8s 967us/step - loss: 0.4028 - acc: 0.8341\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 8s 942us/step - loss: 0.4036 - acc: 0.8359\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 8s 956us/step - loss: 0.4022 - acc: 0.8331 1s - \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 8s 969us/step - loss: 0.4027 - acc: 0.8331 0s - loss: 0.4030 - acc: 0\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 8s 943us/step - loss: 0.4028 - acc: 0.8336\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 8s 943us/step - loss: 0.4020 - acc: 0.8345\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 8s 974us/step - loss: 0.4026 - acc: 0.8335 0s - loss: 0.4\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 8s 970us/step - loss: 0.4028 - acc: 0.8354\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 8s 953us/step - loss: 0.4034 - acc: 0.8341\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 8s 942us/step - loss: 0.4028 - acc: 0.8346\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 8s 963us/step - loss: 0.4027 - acc: 0.8335\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 8s 964us/step - loss: 0.4035 - acc: 0.8340\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 8s 939us/step - loss: 0.4028 - acc: 0.8326\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 8s 941us/step - loss: 0.4034 - acc: 0.8329\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 8s 974us/step - loss: 0.4022 - acc: 0.8346\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 8s 942us/step - loss: 0.4032 - acc: 0.8346\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 8s 959us/step - loss: 0.4023 - acc: 0.8336\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 8s 961us/step - loss: 0.4030 - acc: 0.8339\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 8s 938us/step - loss: 0.4018 - acc: 0.8317\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 8s 938us/step - loss: 0.4031 - acc: 0.8341\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 8s 958us/step - loss: 0.4031 - acc: 0.8345\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 8s 962us/step - loss: 0.4026 - acc: 0.8342\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 8s 938us/step - loss: 0.4027 - acc: 0.8327\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 8s 940us/step - loss: 0.4031 - acc: 0.8341\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 8s 964us/step - loss: 0.4023 - acc: 0.8336- ETA: 0s - loss: 0.397\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4025 - acc: 0.8317\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 8s 946us/step - loss: 0.4014 - acc: 0.8344\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 8s 968us/step - loss: 0.4030 - acc: 0.8350: 5s  - ETA: 2s - loss: 0.4082 -  - ETA\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 12s 1ms/step - loss: 0.4026 - acc: 0.8345\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.4024 - acc: 0.8357\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4027 - acc: 0.8354: 0s - loss: 0.4046\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 12s 2ms/step - loss: 0.4022 - acc: 0.8344\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4031 - acc: 0.8347: 2s - loss: - ETA: 1s -  - ETA: 0s - loss: 0.4036 - acc: 0.834\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 0.4022 - acc: 0.8372\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4023 - acc: 0.8353: 2s -  - ETA: 1s - loss: 0.405 - ETA: 0s - loss: 0.4030 - acc: 0.8\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4021 - acc: 0.8346\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4022 - acc: 0.8334: 4s - los - ETA: 3s - loss: 0.3972 - acc: 0. - ETA: 3s - los\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4024 - acc: 0.8332: 7s - loss:\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 0.4022 - acc: 0.8314\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 599s 75ms/step - loss: 0.4024 - acc: 0.8349\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.4024 - acc: 0.8339\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4024 - acc: 0.8339\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4028 - acc: 0.8344\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 14s 2ms/step - loss: 0.4019 - acc: 0.8334\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 8s 949us/step - loss: 0.4027 - acc: 0.8350\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 8s 954us/step - loss: 0.4020 - acc: 0.8350\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 8s 955us/step - loss: 0.4017 - acc: 0.8335\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 8s 948us/step - loss: 0.4029 - acc: 0.8349\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 7s 920us/step - loss: 0.4025 - acc: 0.8341\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 8s 950us/step - loss: 0.4022 - acc: 0.8356 3s - loss: 0.3962 - acc: 0 - ET\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 7s 927us/step - loss: 0.4027 - acc: 0.8347\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 8s 944us/step - loss: 0.4023 - acc: 0.8351\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 7s 931us/step - loss: 0.4023 - acc: 0.8346 3s - lo\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 7s 928us/step - loss: 0.4022 - acc: 0.8349\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 7s 937us/step - loss: 0.4023 - acc: 0.8345\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 8s 949us/step - loss: 0.4024 - acc: 0.8363\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 7s 920us/step - loss: 0.4026 - acc: 0.8353\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 7s 926us/step - loss: 0.4024 - acc: 0.8342\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 8s 956us/step - loss: 0.4013 - acc: 0.8351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History object at 0x1a159622e8>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"BatchSize\" is the number of observations after which we want to update the weights\n",
    "# BatchSize = 10 (For Stochastic Gradient Descent, it is small batches for which the weights are adjusted)\n",
    "# Epoch -> 1 full interation of all observations over the whole ANN\n",
    "classifier.fit( X_train , Y_train , batch_size=1 ,epochs=100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Test Set results\n",
    "    Although we have obtained the accuracy after each Epoch and we know the accuracy of the predictions on the Training set, We need to see how the classifier performs on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17325348],\n",
       "       [0.3658588 ],\n",
       "       [0.19470961],\n",
       "       ...,\n",
       "       [0.21627688],\n",
       "       [0.1456406 ],\n",
       "       [0.12692833]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,pred in enumerate(Y_pred):\n",
    "        Y_pred[i] = 1 if pred >= 0.5 else 0\n",
    "        \n",
    "cm = confusion_matrix(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1533,   62],\n",
       "       [ 252,  153]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.843"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][0]+cm[0][1]+cm[1][1])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We got an accuracy of 84.3 %\n",
    "    accuracy = number of correct predictions/sum of all prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
